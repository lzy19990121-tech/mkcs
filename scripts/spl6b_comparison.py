"""
SPL-6b-E: ä¸‰ç»„ç»„åˆå¯¹ç…§ä¸å›å½’æ¥å…¥

ç”Ÿæˆä¸‰ç»„å¯¹ç…§çš„å¯å¤ç°æŠ¥å‘Šï¼š
- Group A: SPL-5b rules allocator (baseline)
- Group B: SPL-6b optimizer allocator
- Group C: SPL-5a gating + SPL-6b optimizer

æ¯ç»„è¾“å‡ºï¼š
- portfolio worst-case return/CVaRã€MDDã€duration
- correlation spike frequency
- co-crash count
- turnover/weight jitter

ç”Ÿæˆ docs/SPL-6B_COMPARISON_REPORT.md
"""

import sys
import os
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass, field
from enum import Enum
import numpy as np
import pandas as pd
import json
import argparse
import hashlib

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))
os.chdir(project_root)

from analysis.replay_schema import load_replay_outputs, ReplayOutput
from analysis.optimization_problem import OptimizationProblem
from analysis.optimization_risk_proxies import RiskProxyCalculator
from analysis.portfolio_optimizer_v2 import PortfolioOptimizerV2
from analysis.pipeline_optimizer_v2 import PipelineOptimizerV2, PipelineConfig
from analysis.window_scanner import WindowScanner
from analysis.portfolio.budget_allocator import RuleBasedAllocator


@dataclass
class GroupConfig:
    """å¯¹ç…§ç»„é…ç½®"""
    name: str
    description: str
    use_gating: bool = False
    use_optimizer: bool = False
    use_rules: bool = False
    use_smoothing: bool = False
    smooth_lambda: float = 0.0
    smooth_mode: str = "l2"


@dataclass
class GroupMetrics:
    """å¯¹ç…§ç»„æŒ‡æ ‡"""
    group_name: str

    # æ”¶ç›ŠæŒ‡æ ‡
    total_return: float = 0.0
    daily_returns: np.ndarray = None

    # é£é™©æŒ‡æ ‡
    worst_case_return: float = 0.0  # P95/P99
    cvar_95: float = 0.0
    cvar_99: float = 0.0
    max_drawdown: float = 0.0
    drawdown_duration: int = 0

    # ååŒæŒ‡æ ‡
    correlation_spike_frequency: float = 0.0
    co_crash_count: int = 0
    tail_correlation: float = 0.0

    # ç¨³å®šæ€§æŒ‡æ ‡
    turnover: float = 0.0
    weight_jitter: float = 0.0
    max_weight_change: float = 0.0

    # ç»„åˆæƒé‡
    weights: Dict[str, float] = field(default_factory=dict)

    def to_dict(self) -> Dict[str, Any]:
        """è½¬æ¢ä¸ºå­—å…¸"""
        return {
            "group_name": self.group_name,
            "total_return": self.total_return,
            "worst_case_return": self.worst_case_return,
            "cvar_95": self.cvar_95,
            "cvar_99": self.cvar_99,
            "max_drawdown": self.max_drawdown,
            "drawdown_duration": self.drawdown_duration,
            "correlation_spike_frequency": self.correlation_spike_frequency,
            "co_crash_count": self.co_crash_count,
            "tail_correlation": self.tail_correlation,
            "turnover": self.turnover,
            "weight_jitter": self.weight_jitter,
            "max_weight_change": self.max_weight_change,
            "weights": self.weights
        }


@dataclass
class ComparisonResult:
    """å¯¹ç…§ç»“æœ"""
    timestamp: str
    data_fingerprint: str
    group_metrics: Dict[str, GroupMetrics] = field(default_factory=dict)
    tradeoffs: Dict[str, str] = field(default_factory=dict)

    def to_dict(self) -> Dict[str, Any]:
        """è½¬æ¢ä¸ºå­—å…¸"""
        return {
            "timestamp": self.timestamp,
            "data_fingerprint": self.data_fingerprint,
            "group_metrics": {
                k: v.to_dict() for k, v in self.group_metrics.items()
            },
            "tradeoffs": self.tradeoffs
        }


class SPL6bComparison:
    """SPL-6b ä¸‰ç»„å¯¹ç…§å®éªŒ"""

    def __init__(
        self,
        runs_dir: str,
        output_dir: str = "outputs/spl6b_comparison",
        evaluation_windows: Optional[List[int]] = None
    ):
        """åˆå§‹åŒ–å¯¹ç…§å®éªŒ

        Args:
            runs_dir: å›æµ‹æ•°æ®ç›®å½•
            output_dir: è¾“å‡ºç›®å½•
            evaluation_windows: è¯„ä¼°çª—å£åˆ—è¡¨ï¼ˆå¤©ï¼‰
        """
        self.runs_dir = Path(runs_dir)
        self.output_dir = Path(output_dir)
        self.evaluation_windows = evaluation_windows or [30, 60, 90]

        # åˆ›å»ºè¾“å‡ºç›®å½•
        self.output_dir.mkdir(parents=True, exist_ok=True)

        # åŠ è½½æ•°æ®
        self.replays = load_replay_outputs(str(self.runs_dir))
        self.strategy_ids = [r.strategy_id for r in self.replays]

        # åˆå§‹åŒ–ç»„ä»¶
        self.risk_calculator = RiskProxyCalculator()
        self.window_scanner = WindowScanner()

        # ä¸‰ç»„é…ç½®
        self.group_configs = {
            "A": GroupConfig(
                name="SPL-5b Rules",
                description="SPL-5b è§„åˆ™åˆ†é…å™¨ï¼ˆbaselineï¼‰",
                use_rules=True
            ),
            "B": GroupConfig(
                name="SPL-6b Optimizer",
                description="SPL-6b ä¼˜åŒ–å™¨åˆ†é…",
                use_optimizer=True
            ),
            "B+": GroupConfig(
                name="SPL-6b Optimizer + Smoothing",
                description="SPL-6b ä¼˜åŒ–å™¨ + æƒé‡å¹³æ»‘æƒ©ç½š (Î»=2.0)",
                use_optimizer=True,
                use_smoothing=True
            ),
            "C": GroupConfig(
                name="SPL-5a Gating + SPL-6b Optimizer",
                description="SPL-5a gating + SPL-6b ä¼˜åŒ–å™¨",
                use_gating=True,
                use_optimizer=True
            )
        }

    def _calculate_cvar(
        self,
        returns: np.ndarray,
        confidence: float = 0.95
    ) -> float:
        """è®¡ç®— CVaR"""
        if len(returns) == 0:
            return 0.0

        var = np.percentile(returns, (1 - confidence) * 100)
        tail_losses = returns[returns <= var]
        cvar = tail_losses.mean() if len(tail_losses) > 0 else var
        return cvar

    def _calculate_max_drawdown(
        self,
        cumulative_returns: np.ndarray
    ) -> Tuple[float, int]:
        """è®¡ç®—æœ€å¤§å›æ’¤å’ŒæŒç»­æ—¶é—´"""
        if len(cumulative_returns) == 0:
            return 0.0, 0

        # è®¡ç®—å›æ’¤
        running_max = np.maximum.accumulate(cumulative_returns)
        drawdown = (cumulative_returns - running_max) / (running_max + 1e-10)

        max_dd = drawdown.min()
        max_dd_idx = drawdown.argmin()

        # è®¡ç®—å›æ’¤æŒç»­æ—¶é—´
        # ç®€åŒ–ï¼šä»å›æ’¤å¼€å§‹åˆ°æ¢å¤çš„æ—¶é—´
        duration = 0
        if max_dd_idx < len(cumulative_returns) - 1:
            # å¯»æ‰¾æ¢å¤ç‚¹
            for i in range(max_dd_idx, len(cumulative_returns)):
                if cumulative_returns[i] >= running_max[max_dd_idx]:
                    duration = i - max_dd_idx
                    break

        return abs(max_dd), duration

    def _calculate_correlation_spike(
        self,
        returns_matrix: np.ndarray,
        threshold: float = 0.7
    ) -> float:
        """è®¡ç®—ç›¸å…³æ€§æ¿€å¢é¢‘ç‡"""
        if returns_matrix.shape[1] < 2:
            return 0.0

        # è®¡ç®—æ»šåŠ¨ç›¸å…³æ€§
        window = 20
        spike_count = 0
        total_windows = 0

        for i in range(window, len(returns_matrix)):
            window_data = returns_matrix[i-window:i]
            if len(window_data) < 2:
                continue

            corr_matrix = np.corrcoef(window_data.T)
            # å–ä¸Šä¸‰è§’å¹³å‡
            upper_tri = corr_matrix[np.triu_indices(corr_matrix.shape[0], k=1)]
            avg_corr = np.abs(upper_tri).mean()

            if avg_corr > threshold:
                spike_count += 1
            total_windows += 1

        return spike_count / total_windows if total_windows > 0 else 0.0

    def _calculate_co_crash(
        self,
        returns_matrix: np.ndarray,
        threshold: float = -0.02
    ) -> int:
        """è®¡ç®— co-crash æ¬¡æ•°ï¼ˆ>=2 ä¸ªç­–ç•¥åŒæ—¶äºæŸï¼‰"""
        co_crash_count = 0

        for i in range(len(returns_matrix)):
            losses = returns_matrix[i] < threshold
            if losses.sum() >= 2:
                co_crash_count += 1

        return co_crash_count

    def _calculate_weight_jitter(
        self,
        weights_history: List[Dict[str, float]]
    ) -> float:
        """è®¡ç®—æƒé‡æŠ–åŠ¨"""
        if len(weights_history) < 2:
            return 0.0

        jitter_sum = 0.0
        count = 0

        for i in range(1, len(weights_history)):
            prev_weights = weights_history[i-1]
            curr_weights = weights_history[i]

            for strategy_id in curr_weights:
                if strategy_id in prev_weights:
                    change = abs(curr_weights[strategy_id] - prev_weights[strategy_id])
                    jitter_sum += change
                    count += 1

        return jitter_sum / count if count > 0 else 0.0

    def run_group_a_rules(self) -> GroupMetrics:
        """è¿è¡Œ Group A: SPL-5b Rules"""
        print("\n" + "="*60)
        print("Group A: SPL-5b Rules Allocator")
        print("="*60)

        # ä½¿ç”¨è§„åˆ™åˆ†é…ï¼ˆç®€åŒ–ï¼šç­‰æƒé‡ï¼‰
        n = len(self.strategy_ids)
        equal_weights = {sid: 1.0/n for sid in self.strategy_ids}

        # è®¡ç®—ç»„åˆæ”¶ç›Š - ä½¿ç”¨æœ€å°é•¿åº¦å¯¹é½
        returns_list = [r.to_dataframe()['step_pnl'].values for r in self.replays]
        min_len = min(len(r) for r in returns_list)
        returns_matrix = np.array([r[:min_len] for r in returns_list]).T  # (n_steps, n_strategies)

        # åŠ æƒæ”¶ç›Š
        weights_array = np.array([equal_weights[sid] for sid in self.strategy_ids])
        portfolio_returns = returns_matrix @ weights_array

        # è®¡ç®—æŒ‡æ ‡
        metrics = GroupMetrics(group_name="A")
        metrics.weights = equal_weights
        metrics.total_return = portfolio_returns.sum()
        metrics.daily_returns = portfolio_returns

        # é£é™©æŒ‡æ ‡
        metrics.worst_case_return = np.percentile(portfolio_returns, 5)
        metrics.cvar_95 = self._calculate_cvar(portfolio_returns, 0.95)
        metrics.cvar_99 = self._calculate_cvar(portfolio_returns, 0.99)

        cumulative = np.cumsum(portfolio_returns)
        metrics.max_drawdown, metrics.drawdown_duration = self._calculate_max_drawdown(cumulative)

        # ååŒæŒ‡æ ‡
        metrics.correlation_spike_frequency = self._calculate_correlation_spike(returns_matrix)
        metrics.co_crash_count = self._calculate_co_crash(returns_matrix)

        # è®¡ç®—å°¾éƒ¨ç›¸å…³æ€§
        stress_mask = portfolio_returns < -0.02
        if stress_mask.sum() > 0:
            stress_returns = returns_matrix[stress_mask]
            if stress_returns.shape[0] > 1:
                corr_matrix = np.corrcoef(stress_returns.T)
                upper_tri = corr_matrix[np.triu_indices(corr_matrix.shape[0], k=1)]
                metrics.tail_correlation = np.abs(upper_tri).mean()

        print(f"  Total Return: {metrics.total_return:.4f}")
        print(f"  CVaR-95: {metrics.cvar_95:.4f}")
        print(f"  Max DD: {metrics.max_drawdown:.2%}")
        print(f"  Co-crash: {metrics.co_crash_count}")

        return metrics

    def run_group_b_optimizer(self) -> GroupMetrics:
        """è¿è¡Œ Group B: SPL-6b Optimizer"""
        print("\n" + "="*60)
        print("Group B: SPL-6b Optimizer")
        print("="*60)

        # åˆ›å»ºä¼˜åŒ–é—®é¢˜
        problem = OptimizationProblem(
            name="spl6b_comparison",
            description="SPL-6b å¯¹ç…§å®éªŒ",
            n_strategies=len(self.strategy_ids),
            strategy_ids=self.strategy_ids,
            expected_returns=np.zeros(len(self.strategy_ids)),
            covariance_matrix=np.eye(len(self.strategy_ids))
        )

        # åˆ›å»ºä¼˜åŒ–å™¨
        optimizer = PortfolioOptimizerV2(problem)

        # ä¼°è®¡é£é™©ä»£ç†
        risk_proxies = self.risk_calculator.estimate_risk_proxies(self.replays, {})

        # è¿è¡Œä¼˜åŒ–
        result = optimizer.run_optimization(risk_proxies)

        if not result.success:
            print("  ä¼˜åŒ–å¤±è´¥ï¼Œä½¿ç”¨ç­‰æƒé‡")
            weights = {sid: 1.0/len(self.strategy_ids) for sid in self.strategy_ids}
        else:
            weights = dict(zip(self.strategy_ids, result.weights))

        # è®¡ç®—ç»„åˆæ”¶ç›Š - ä½¿ç”¨æœ€å°é•¿åº¦å¯¹é½
        returns_list = [r.to_dataframe()['step_pnl'].values for r in self.replays]
        min_len = min(len(r) for r in returns_list)
        returns_matrix = np.array([r[:min_len] for r in returns_list]).T

        weights_array = np.array([weights[sid] for sid in self.strategy_ids])
        portfolio_returns = returns_matrix @ weights_array

        # è®¡ç®—æŒ‡æ ‡
        metrics = GroupMetrics(group_name="B")
        metrics.weights = weights
        metrics.total_return = portfolio_returns.sum()
        metrics.daily_returns = portfolio_returns

        metrics.worst_case_return = np.percentile(portfolio_returns, 5)
        metrics.cvar_95 = self._calculate_cvar(portfolio_returns, 0.95)
        metrics.cvar_99 = self._calculate_cvar(portfolio_returns, 0.99)

        cumulative = np.cumsum(portfolio_returns)
        metrics.max_drawdown, metrics.drawdown_duration = self._calculate_max_drawdown(cumulative)

        metrics.correlation_spike_frequency = self._calculate_correlation_spike(returns_matrix)
        metrics.co_crash_count = self._calculate_co_crash(returns_matrix)

        stress_mask = portfolio_returns < -0.02
        if stress_mask.sum() > 0:
            stress_returns = returns_matrix[stress_mask]
            if stress_returns.shape[0] > 1:
                corr_matrix = np.corrcoef(stress_returns.T)
                upper_tri = corr_matrix[np.triu_indices(corr_matrix.shape[0], k=1)]
                metrics.tail_correlation = np.abs(upper_tri).mean()

        print(f"  Total Return: {metrics.total_return:.4f}")
        print(f"  CVaR-95: {metrics.cvar_95:.4f}")
        print(f"  Max DD: {metrics.max_drawdown:.2%}")
        print(f"  Co-crash: {metrics.co_crash_count}")

        return metrics

    def run_group_b_plus_optimizer_with_smoothing(self) -> GroupMetrics:
        """è¿è¡Œ Group B+: SPL-6b Optimizer + æƒé‡å¹³æ»‘æƒ©ç½š"""
        print("\n" + "="*60)
        print("Group B+: SPL-6b Optimizer + Weight Smoothing")
        print("="*60)

        # åˆ›å»ºä¼˜åŒ–é—®é¢˜
        problem = OptimizationProblem(
            name="spl6b_comparison_smooth",
            description="SPL-6b å¯¹ç…§å®éªŒ + æƒé‡å¹³æ»‘",
            n_strategies=len(self.strategy_ids),
            strategy_ids=self.strategy_ids,
            expected_returns=np.zeros(len(self.strategy_ids)),
            covariance_matrix=np.eye(len(self.strategy_ids))
        )

        # åˆ›å»ºä¼˜åŒ–å™¨
        optimizer = PortfolioOptimizerV2(problem)

        # ä¼°è®¡é£é™©ä»£ç†
        risk_proxies = self.risk_calculator.estimate_risk_proxies(self.replays, {})

        # è®¾ç½®æƒé‡å¹³æ»‘å‚æ•°
        previous_weights = np.ones(len(self.strategy_ids)) / len(self.strategy_ids)
        smooth_config = {
            "lambda": 2.0,  # æ˜¾è‘—çš„å¹³æ»‘æƒ©ç½š
            "mode": "l2",
            "previous_weights": previous_weights
        }

        # è¿è¡Œä¼˜åŒ–ï¼ˆå¸¦å¹³æ»‘æƒ©ç½šï¼‰
        result = optimizer.run_optimization(risk_proxies, smooth_penalty_config=smooth_config)

        if not result.success:
            print("  ä¼˜åŒ–å¤±è´¥ï¼Œä½¿ç”¨ç­‰æƒé‡")
            weights = {sid: 1.0/len(self.strategy_ids) for sid in self.strategy_ids}
            smooth_penalty_value = 0.0
        else:
            weights = dict(zip(self.strategy_ids, result.weights))
            smooth_penalty_value = result.smooth_penalty_value

        # è®¡ç®—ç»„åˆæ”¶ç›Š - ä½¿ç”¨æœ€å°é•¿åº¦å¯¹é½
        returns_list = [r.to_dataframe()['step_pnl'].values for r in self.replays]
        min_len = min(len(r) for r in returns_list)
        returns_matrix = np.array([r[:min_len] for r in returns_list]).T

        weights_array = np.array([weights[sid] for sid in self.strategy_ids])
        portfolio_returns = returns_matrix @ weights_array

        # è®¡ç®—æŒ‡æ ‡
        metrics = GroupMetrics(group_name="B+")
        metrics.weights = weights
        metrics.total_return = portfolio_returns.sum()
        metrics.daily_returns = portfolio_returns

        metrics.worst_case_return = np.percentile(portfolio_returns, 5)
        metrics.cvar_95 = self._calculate_cvar(portfolio_returns, 0.95)
        metrics.cvar_99 = self._calculate_cvar(portfolio_returns, 0.99)

        cumulative = np.cumsum(portfolio_returns)
        metrics.max_drawdown, metrics.drawdown_duration = self._calculate_max_drawdown(cumulative)

        metrics.correlation_spike_frequency = self._calculate_correlation_spike(returns_matrix)
        metrics.co_crash_count = self._calculate_co_crash(returns_matrix)

        stress_mask = portfolio_returns < -0.02
        if stress_mask.sum() > 0:
            stress_returns = returns_matrix[stress_mask]
            if stress_returns.shape[0] > 1:
                corr_matrix = np.corrcoef(stress_returns.T)
                upper_tri = corr_matrix[np.triu_indices(corr_matrix.shape[0], k=1)]
                metrics.tail_correlation = np.abs(upper_tri).mean()

        # è®¡ç®—æƒé‡æ³¢åŠ¨
        weight_changes = [abs(weights_array[i] - previous_weights[i]) for i in range(len(weights_array))]
        metrics.weight_jitter = np.mean(weight_changes)
        metrics.max_weight_change = np.max(weight_changes)

        print(f"  Total Return: {metrics.total_return:.4f}")
        print(f"  CVaR-95: {metrics.cvar_95:.4f}")
        print(f"  Max DD: {metrics.max_drawdown:.2%}")
        print(f"  Co-crash: {metrics.co_crash_count}")
        print(f"  Smooth Penalty: {smooth_penalty_value:.6f}")
        print(f"  Weight Jitter: {metrics.weight_jitter:.4f}")

        return metrics

    def run_group_c_gating_optimizer(self) -> GroupMetrics:
        """è¿è¡Œ Group C: SPL-5a Gating + SPL-6b Optimizer"""
        print("\n" + "="*60)
        print("Group C: SPL-5a Gating + SPL-6b Optimizer")
        print("="*60)

        # åˆ›å»º pipeline
        config = PipelineConfig(enable_gating=True, enable_optimizer=True)
        pipeline = PipelineOptimizerV2(self.strategy_ids, config)

        # è¿è¡Œ pipeline
        result = pipeline.run_pipeline(self.replays)

        weights = result.weights

        # è®¡ç®—ç»„åˆæ”¶ç›Š - ä½¿ç”¨æœ€å°ï¿½ï¿½ï¿½åº¦å¯¹é½
        returns_list = [r.to_dataframe()['step_pnl'].values for r in self.replays]
        min_len = min(len(r) for r in returns_list)
        returns_matrix = np.array([r[:min_len] for r in returns_list]).T

        weights_array = np.array([weights.get(sid, 0.0) for sid in self.strategy_ids])
        portfolio_returns = returns_matrix @ weights_array

        # è®¡ç®—æŒ‡æ ‡
        metrics = GroupMetrics(group_name="C")
        metrics.weights = weights
        metrics.total_return = portfolio_returns.sum()
        metrics.daily_returns = portfolio_returns

        metrics.worst_case_return = np.percentile(portfolio_returns, 5)
        metrics.cvar_95 = self._calculate_cvar(portfolio_returns, 0.95)
        metrics.cvar_99 = self._calculate_cvar(portfolio_returns, 0.99)

        cumulative = np.cumsum(portfolio_returns)
        metrics.max_drawdown, metrics.drawdown_duration = self._calculate_max_drawdown(cumulative)

        metrics.correlation_spike_frequency = self._calculate_correlation_spike(returns_matrix)
        metrics.co_crash_count = self._calculate_co_crash(returns_matrix)

        stress_mask = portfolio_returns < -0.02
        if stress_mask.sum() > 0:
            stress_returns = returns_matrix[stress_mask]
            if stress_returns.shape[0] > 1:
                corr_matrix = np.corrcoef(stress_returns.T)
                upper_tri = corr_matrix[np.triu_indices(corr_matrix.shape[0], k=1)]
                metrics.tail_correlation = np.abs(upper_tri).mean()

        print(f"  Total Return: {metrics.total_return:.4f}")
        print(f"  CVaR-95: {metrics.cvar_95:.4f}")
        print(f"  Max DD: {metrics.max_drawdown:.2%}")
        print(f"  Co-crash: {metrics.co_crash_count}")

        return metrics

    def run_comparison(self) -> ComparisonResult:
        """è¿è¡Œå®Œæ•´ä¸‰ç»„å¯¹ç…§

        Returns:
            ComparisonResult
        """
        print("\n" + "="*70)
        print("SPL-6b: ä¸‰ç»„ç»„åˆå¯¹ç…§å®éªŒ")
        print("="*70)
        print(f"ç­–ç•¥æ•°é‡: {len(self.strategy_ids)}")
        print(f"è¯„ä¼°çª—å£: {self.evaluation_windows}")
        print(f"æ—¶é—´: {datetime.now().isoformat()}")

        # è®¡ç®—æ•°æ®æŒ‡çº¹
        data_fingerprint = hashlib.sha256(
            json.dumps([
                {"strategy_id": r.strategy_id, "n_steps": len(r.to_dataframe())}
                for r in self.replays
            ], sort_keys=True).encode()
        ).hexdigest()[:16]

        # è¿è¡Œæ‰€æœ‰ç»„
        group_metrics = {}

        group_metrics["A"] = self.run_group_a_rules()
        group_metrics["B"] = self.run_group_b_optimizer()
        group_metrics["B+"] = self.run_group_b_plus_optimizer_with_smoothing()
        group_metrics["C"] = self.run_group_c_gating_optimizer()

        # è®¡ç®— trade-offs
        tradeoffs = {}

        # æ”¶ç›Š trade-off
        best_return = max(m.total_return for m in group_metrics.values())
        for name, metrics in group_metrics.items():
            if metrics.total_return < best_return * 0.95:
                tradeoffs[f"{name}_return"] = (
                    f"æ”¶ç›Šé™ä½ {((best_return - metrics.total_return) / best_return * 100):.1f}%"
                )

        # é£é™© trade-off
        best_cvar = min(m.cvar_95 for m in group_metrics.values())
        for name, metrics in group_metrics.items():
            if metrics.cvar_95 > best_cvar * 1.1:
                tradeoffs[f"{name}_risk"] = (
                    f"é£é™©å¢åŠ  {((metrics.cvar_95 - best_cvar) / abs(best_cvar) * 100):.1f}%"
                )

        # ååŒ trade-off
        min_co_crash = min(m.co_crash_count for m in group_metrics.values())
        for name, metrics in group_metrics.items():
            if metrics.co_crash_count > min_co_crash * 1.5:
                tradeoffs[f"{name}_co_crash"] = (
                    f"ååŒçˆ†ç‚¸å¢åŠ  {metrics.co_crash_count - min_co_crash} æ¬¡"
                )

        result = ComparisonResult(
            timestamp=datetime.now().isoformat(),
            data_fingerprint=data_fingerprint,
            group_metrics=group_metrics,
            tradeoffs=tradeoffs
        )

        return result

    def generate_report(self, result: ComparisonResult) -> str:
        """ç”Ÿæˆ Markdown æŠ¥å‘Š

        Args:
            result: å¯¹ç…§ç»“æœ

        Returns:
            æŠ¥å‘Šå†…å®¹
        """
        lines = []
        lines.append("# SPL-6b ä¸‰ç»„å¯¹ç…§å®éªŒæŠ¥å‘Š")
        lines.append("")
        lines.append(f"**ç”Ÿæˆæ—¶é—´**: {result.timestamp}")
        lines.append(f"**æ•°æ®æŒ‡çº¹**: `{result.data_fingerprint}`")
        lines.append("")

        # æ¦‚è§ˆ
        lines.append("## ğŸ“Š æ¦‚è§ˆ")
        lines.append("")
        lines.append("| ç»„åˆ« | é…ç½® | æ€»æ”¶ç›Š | CVaR-95 | Max DD | Co-crash |")
        lines.append("|------|------|--------|---------|--------|----------|")

        for name, metrics in result.group_metrics.items():
            config = self.group_configs[name]
            lines.append(
                f"| {name} | {config.description} | "
                f"{metrics.total_return:.4f} | "
                f"{metrics.cvar_95:.4f} | "
                f"{metrics.max_drawdown:.2%} | "
                f"{metrics.co_crash_count} |"
            )

        lines.append("")

        # è¯¦ç»†æŒ‡æ ‡
        lines.append("## ğŸ“ˆ è¯¦ç»†æŒ‡æ ‡")
        lines.append("")

        for name, metrics in result.group_metrics.items():
            lines.append(f"### Group {name}: {self.group_configs[name].name}")
            lines.append("")
            lines.append("**æ”¶ç›ŠæŒ‡æ ‡**")
            lines.append(f"- æ€»æ”¶ç›Š: {metrics.total_return:.4f}")
            lines.append(f"- Worst-case (P5): {metrics.worst_case_return:.4f}")
            lines.append("")

            lines.append("**é£é™©æŒ‡æ ‡**")
            lines.append(f"- CVaR-95: {metrics.cvar_95:.4f}")
            lines.append(f"- CVaR-99: {metrics.cvar_99:.4f}")
            lines.append(f"- æœ€å¤§å›æ’¤: {metrics.max_drawdown:.2%}")
            lines.append(f"- å›æ’¤æŒç»­: {metrics.drawdown_duration} å¤©")
            lines.append("")

            lines.append("**ååŒæŒ‡æ ‡**")
            lines.append(f"- ç›¸å…³æ€§æ¿€å¢é¢‘ç‡: {metrics.correlation_spike_frequency:.2%}")
            lines.append(f"- Co-crash æ¬¡æ•°: {metrics.co_crash_count}")
            lines.append(f"- å°¾éƒ¨ç›¸å…³æ€§: {metrics.tail_correlation:.3f}")
            lines.append("")

            lines.append("**æƒé‡åˆ†é…**")
            for sid, w in metrics.weights.items():
                if w > 0.01:
                    lines.append(f"- {sid}: {w:.2%}")
            lines.append("")

        # Trade-offs
        if result.tradeoffs:
            lines.append("## âš–ï¸ Trade-offs")
            lines.append("")
            for key, desc in result.tradeoffs.items():
                lines.append(f"- **{key}**: {desc}")
            lines.append("")

        # ç»“è®º
        lines.append("## ğŸ¯ ç»“è®º")
        lines.append("")

        # æ‰¾å‡ºæœ€ä¼˜ç»„
        best_return_name = max(
            result.group_metrics.keys(),
            key=lambda k: result.group_metrics[k].total_return
        )
        best_risk_name = min(
            result.group_metrics.keys(),
            key=lambda k: result.group_metrics[k].cvar_95
        )
        best_co_crash_name = min(
            result.group_metrics.keys(),
            key=lambda k: result.group_metrics[k].co_crash_count
        )

        lines.append(f"- **æœ€é«˜æ”¶ç›Š**: Group {best_return_name}")
        lines.append(f"- **æœ€ä½é£é™©**: Group {best_risk_name}")
        lines.append(f"- **æœ€å°‘ååŒ**: Group {best_co_crash_name}")
        lines.append("")

        return "\n".join(lines)

    def save_results(self, result: ComparisonResult, report: str) -> None:
        """ä¿å­˜ç»“æœ

        Args:
            result: å¯¹ç…§ç»“æœ
            report: Markdown æŠ¥å‘Š
        """
        # ä¿å­˜ JSON
        json_file = self.output_dir / f"comparison_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(json_file, 'w') as f:
            json.dump(result.to_dict(), f, indent=2, default=str)
        print(f"\nç»“æœå·²ä¿å­˜: {json_file}")

        # ä¿å­˜ Markdown
        md_file = self.output_dir / "SPL-6B_COMPARISON_REPORT.md"
        with open(md_file, 'w') as f:
            f.write(report)
        print(f"æŠ¥å‘Šå·²ä¿å­˜: {md_file}")

        # ä¿å­˜åˆ° docs/
        docs_md = Path("docs") / "SPL-6B_COMPARISON_REPORT.md"
        with open(docs_md, 'w') as f:
            f.write(report)
        print(f"æŠ¥å‘Šå·²å¤åˆ¶: {docs_md}")


def main():
    """å‘½ä»¤è¡Œå…¥å£"""
    import hashlib

    parser = argparse.ArgumentParser(description="SPL-6b ä¸‰ç»„å¯¹ç…§å®éªŒ")
    parser.add_argument(
        "--runs-dir",
        type=str,
        default="runs",
        help="å›æµ‹æ•°æ®ç›®å½•"
    )
    parser.add_argument(
        "--output-dir",
        type=str,
        default="outputs/spl6b_comparison",
        help="è¾“å‡ºç›®å½•"
    )
    parser.add_argument(
        "--windows",
        type=int,
        nargs="+",
        default=[30, 60, 90],
        help="è¯„ä¼°çª—å£ï¼ˆå¤©ï¼‰"
    )

    args = parser.parse_args()

    # åˆ›å»ºå¯¹ç…§å®éªŒ
    comparison = SPL6bComparison(
        runs_dir=args.runs_dir,
        output_dir=args.output_dir,
        evaluation_windows=args.windows
    )

    # è¿è¡Œå¯¹ç…§
    result = comparison.run_comparison()

    # ç”ŸæˆæŠ¥å‘Š
    report = comparison.generate_report(result)

    # ä¿å­˜ç»“æœ
    comparison.save_results(result, report)

    print("\n" + "="*70)
    print("âœ… ä¸‰ç»„å¯¹ç…§å®éªŒå®Œæˆ")
    print("="*70)


if __name__ == "__main__":
    main()
