#!/usr/bin/env python3
"""
MKCS A/B æµ‹è¯•æ¡†æ¶ (T3-1)

ä¸€é”®è¿è¡Œä¸¤ä¸ªå›æµ‹é…ç½®å¹¶ç”Ÿæˆå¯¹ç…§æŠ¥å‘Š
"""

import sys
import os
from pathlib import Path
from datetime import datetime
import json
import logging
import argparse
from typing import Dict, List
import subprocess

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))
os.chdir(project_root)

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class ABTestRunner:
    """A/B æµ‹è¯•è¿è¡Œå™¨"""

    def __init__(
        self,
        symbols: List[str],
        start_date: str,
        end_date: str,
        output_dir: str = "outputs/backtests",
        baseline_config: Dict = None,
        variant_config: Dict = None
    ):
        self.symbols = symbols
        self.start_date = start_date
        self.end_date = end_date
        self.output_dir = Path(output_dir)
        self.baseline_config = baseline_config or {}
        self.variant_config = variant_config or {}

        # é»˜è®¤é…ç½®
        self.default_config = {
            'capital': 100000,
            'max_risk': 0.02,
            'vote_threshold': 2,
            'min_strength': 0.001,
            'conflict_mode': 'HOLD'
        }

    def run_backtest(self, config: Dict, label: str) -> Dict:
        """è¿è¡Œå•ä¸ªå›æµ‹"""
        # åˆå¹¶é…ç½®
        full_config = {**self.default_config, **config}

        # ç”Ÿæˆ run_id
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        symbols_str = "_".join(self.symbols[:3])
        run_id = f"ab_{label}_{symbols_str}_{timestamp}"

        # æ„å»ºå‘½ä»¤
        cmd = [
            sys.executable,
            "scripts/run_backtest_strict.py",
            "--symbols", *self.symbols,
            "--start", self.start_date,
            "--end", self.end_date,
            "--capital", str(full_config['capital']),
            "--max-risk", str(full_config['max_risk']),
            "--vote-threshold", str(full_config['vote_threshold']),
            "--min-strength", str(full_config['min_strength']),
            "--conflict-mode", full_config['conflict_mode'],
            "--output-dir", str(self.output_dir),
            "--run-id", run_id
        ]

        logger.info(f"è¿è¡Œ {label} å›æµ‹...")
        logger.info(f"å‘½ä»¤: {' '.join(cmd)}")

        # è¿è¡Œå›æµ‹
        result = subprocess.run(
            cmd,
            capture_output=True,
            text=True,
            cwd=project_root
        )

        if result.returncode != 0:
            logger.error(f"{label} å›æµ‹å¤±è´¥!")
            logger.error(result.stderr)
            return None

        # è¯»å–ç»“æœ
        run_dir = self.output_dir / run_id
        summary_path = run_dir / "summary.json"

        if not summary_path.exists():
            logger.error(f"{label} å›æµ‹æœªç”Ÿæˆ summary.json")
            return None

        with open(summary_path) as f:
            summary = json.load(f)

        summary['run_dir'] = str(run_dir)
        summary['label'] = label

        logger.info(f"âœ… {label} å›æµ‹å®Œæˆ")
        return summary

    def run_comparison(self) -> Dict:
        """è¿è¡Œ A/B å¯¹æ¯”"""
        logger.info("=" * 60)
        logger.info("MKCS A/B æµ‹è¯•æ¡†æ¶")
        logger.info("=" * 60)
        logger.info(f"æ ‡çš„: {', '.join(self.symbols)}")
        logger.info(f"åŒºé—´: {self.start_date} ~ {self.end_date}")

        # è¿è¡Œ baseline
        baseline = self.run_backtest(self.baseline_config, "baseline")
        if not baseline:
            return {'success': False, 'error': 'Baseline failed'}

        # è¿è¡Œ variant
        variant = self.run_backtest(self.variant_config, "variant")
        if not variant:
            return {'success': False, 'error': 'Variant failed'}

        # ç”Ÿæˆå¯¹ç…§æŠ¥å‘Š
        return self._generate_report(baseline, variant)

    def _generate_report(self, baseline: Dict, variant: Dict) -> Dict:
        """ç”Ÿæˆå¯¹ç…§æŠ¥å‘Š"""
        logger.info("\n" + "=" * 60)
        logger.info("ç”Ÿæˆ A/B å¯¹ç…§æŠ¥å‘Š")
        logger.info("=" * 60)

        # è®¡ç®—å·®å¼‚
        report = {
            'test_type': 'AB Comparison',
            'timestamp': datetime.now().isoformat(),
            'symbols': self.symbols,
            'period': f"{self.start_date} ~ {self.end_date}",
            'baseline': {
                'label': 'baseline',
                'config': self.baseline_config,
                'metrics': self._extract_metrics(baseline)
            },
            'variant': {
                'label': 'variant',
                'config': self.variant_config,
                'metrics': self._extract_metrics(variant)
            },
            'comparison': self._compare_metrics(baseline, variant)
        }

        # ä¿å­˜æŠ¥å‘Š
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        report_path = self.output_dir / f"ab_comparison_{timestamp}.json"
        with open(report_path, 'w') as f:
            json.dump(report, f, indent=2, default=str)

        # ç”Ÿæˆ Markdown æŠ¥å‘Š
        md_path = self._generate_markdown_report(report)

        logger.info(f"\nâœ… å¯¹ç…§æŠ¥å‘Šå·²ä¿å­˜:")
        logger.info(f"  - JSON: {report_path}")
        logger.info(f"  - Markdown: {md_path}")

        return {**report, 'success': True, 'report_path': str(report_path), 'md_path': str(md_path)}

    def _extract_metrics(self, result: Dict) -> Dict:
        """æå–å…³é”®æŒ‡æ ‡"""
        return {
            'total_return_pct': result.get('total_return_pct', 0),
            'max_drawdown_pct': result.get('max_drawdown_pct', 0),
            'sharpe_ratio': result.get('sharpe_ratio', 0),
            'total_trades': result.get('total_trades', 0),
            'win_rate': result.get('win_rate', 0),
            'exit_reasons': result.get('exit_reasons', {})
        }

    def _compare_metrics(self, baseline: Dict, variant: Dict) -> Dict:
        """å¯¹æ¯”æŒ‡æ ‡"""
        b = self._extract_metrics(baseline)
        v = self._extract_metrics(variant)

        return {
            'return_diff': v['total_return_pct'] - b['total_return_pct'],
            'return_improvement': (v['total_return_pct'] - b['total_return_pct']) / abs(b['total_return_pct']) * 100 if b['total_return_pct'] != 0 else 0,
            'drawdown_diff': v['max_drawdown_pct'] - b['max_drawdown_pct'],
            'drawdown_improvement': (b['max_drawdown_pct'] - v['max_drawdown_pct']) / b['max_drawdown_pct'] * 100 if b['max_drawdown_pct'] != 0 else 0,
            'sharpe_diff': v['sharpe_ratio'] - b['sharpe_ratio'],
            'trades_diff': v['total_trades'] - b['total_trades'],
            'winner': 'variant' if v['total_return_pct'] > b['total_return_pct'] else 'baseline'
        }

    def _generate_markdown_report(self, report: Dict) -> Path:
        """ç”Ÿæˆ Markdown æŠ¥å‘Š"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        md_path = self.output_dir / f"AB_COMPARISON_{timestamp}.md"

        b = report['baseline']['metrics']
        v = report['variant']['metrics']
        c = report['comparison']

        with open(md_path, 'w') as f:
            f.write("# MKCS A/B æµ‹è¯•å¯¹ç…§æŠ¥å‘Š\n\n")
            f.write(f"**ç”Ÿæˆæ—¶é—´**: {report['timestamp']}\n")
            f.write(f"**æµ‹è¯•æœŸé—´**: {report['period']}\n")
            f.write(f"**äº¤æ˜“æ ‡çš„**: {', '.join(report['symbols'])}\n\n")

            f.write("## ğŸ“Š é…ç½®å¯¹æ¯”\n\n")
            f.write("| å‚æ•° | Baseline | Variant |\n")
            f.write("|------|----------|--------|\n")

            for key in set(list(report['baseline']['config'].keys()) + list(report['variant']['config'].keys())):
                b_val = report['baseline']['config'].get(key, '-')
                v_val = report['variant']['config'].get(key, '-')
                f.write(f"| {key} | {b_val} | {v_val} |\n")

            f.write("\n## ğŸ“ˆ æ€§èƒ½æŒ‡æ ‡å¯¹æ¯”\n\n")
            f.write("| æŒ‡æ ‡ | Baseline | Variant | å·®å¼‚ |\n")
            f.write("|------|----------|---------|------|\n")
            f.write(f"| æ€»æ”¶ç›Š (%) | {b['total_return_pct']:.2f} | {v['total_return_pct']:.2f} | {c['return_diff']:+.2f} |\n")
            f.write(f"| æœ€å¤§å›æ’¤ (%) | {b['max_drawdown_pct']:.2f} | {v['max_drawdown_pct']:.2f} | {c['drawdown_diff']:+.2f} |\n")
            f.write(f"| å¤æ™®æ¯”ç‡ | {b['sharpe_ratio']:.4f} | {v['sharpe_ratio']:.4f} | {c['sharpe_diff']:+.4f} |\n")
            f.write(f"| æ€»äº¤æ˜“æ•° | {b['total_trades']} | {v['total_trades']} | {c['trades_diff']:+d} |\n")
            f.write(f"| èƒœç‡ (%) | {b['win_rate']*100:.2f} | {v['win_rate']*100:.2f} | {(v['win_rate']-b['win_rate'])*100:+.2f} |\n")

            f.write("\n## ğŸ¯ é€€å‡ºåŸå› å¯¹æ¯”\n\n")
            f.write("### Baseline\n\n")
            for reason, count in b['exit_reasons'].items():
                f.write(f"- {reason}: {count}\n")

            f.write("\n### Variant\n\n")
            for reason, count in v['exit_reasons'].items():
                f.write(f"- {reason}: {count}\n")

            f.write("\n## ğŸ† ç»“è®º\n\n")
            if c['winner'] == 'variant':
                f.write(f"- **èƒœè€…**: Variant\n")
                f.write(f"- **æ”¶ç›Šæå‡**: {c['return_improvement']:+.2f}%\n")
                if c['drawdown_improvement'] != 0:
                    f.write(f"- **å›æ’¤æ”¹å–„**: {c['drawdown_improvement']:+.2f}%\n")
            else:
                f.write(f"- **èƒœè€…**: Baseline\n")
                f.write(f"- **æ”¶ç›Šé™ä½**: {c['return_improvement']:+.2f}%\n")

        return md_path


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="MKCS A/B æµ‹è¯•æ¡†æ¶")
    parser.add_argument('--symbols', nargs='+', default=['AAPL', 'MSFT', 'GOOGL'], help='äº¤æ˜“æ ‡çš„')
    parser.add_argument('--start', default='2023-01-01', help='å¼€å§‹æ—¥æœŸ')
    parser.add_argument('--end', default='2024-12-31', help='ç»“æŸæ—¥æœŸ')
    parser.add_argument('--output-dir', default='outputs/backtests', help='è¾“å‡ºç›®å½•')

    # Baseline é…ç½®
    parser.add_argument('--baseline-threshold', type=int, default=2, help='Baseline æŠ•ç¥¨é˜ˆå€¼')
    parser.add_argument('--baseline-strength', type=float, default=0.001, help='Baseline æœ€å°å¼ºåº¦')

    # Variant é…ç½®
    parser.add_argument('--variant-threshold', type=int, default=1, help='Variant æŠ•ç¥¨é˜ˆå€¼')
    parser.add_argument('--variant-strength', type=float, default=0.001, help='Variant æœ€å°å¼ºåº¦')
    parser.add_argument('--variant-conflict', default='HOLD', choices=['HOLD', 'STRENGTH_DIFF'], help='Variant å†²çªæ¨¡å¼')

    args = parser.parse_args()

    # åˆ›å»ºé…ç½®
    baseline_config = {
        'vote_threshold': args.baseline_threshold,
        'min_strength': args.baseline_strength,
        'conflict_mode': 'HOLD'
    }

    variant_config = {
        'vote_threshold': args.variant_threshold,
        'min_strength': args.variant_strength,
        'conflict_mode': args.variant_conflict
    }

    # è¿è¡Œ A/B æµ‹è¯•
    runner = ABTestRunner(
        symbols=args.symbols,
        start_date=args.start,
        end_date=args.end,
        output_dir=args.output_dir,
        baseline_config=baseline_config,
        variant_config=variant_config
    )

    result = runner.run_comparison()

    if result.get('success'):
        logger.info("\nğŸ‰ A/B æµ‹è¯•å®Œæˆ!")
    else:
        logger.error(f"\nâŒ A/B æµ‹è¯•å¤±è´¥: {result.get('error')}")
        sys.exit(1)


if __name__ == "__main__":
    main()
