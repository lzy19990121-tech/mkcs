#!/usr/bin/env python3
"""
è®­ç»ƒ Attention-LSTM æ¨¡å‹

ä½¿ç”¨çœŸå®æ•°æ®è®­ç»ƒå¸¦æœ‰æ³¨æ„åŠ›æœºåˆ¶çš„ LSTM æ¨¡å‹
"""

import logging
from datetime import datetime, timedelta
from pathlib import Path
import numpy as np

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


def train_attention_lstm(
    data_path='data/real_market_data.csv',
    model_path='models/attention_lstm.h5',
    epochs=100,
    sequence_length=30
):
    """è®­ç»ƒ Attention-LSTM æ¨¡å‹

    Args:
        data_path: æ•°æ®æ–‡ä»¶è·¯å¾„
        model_path: æ¨¡å‹ä¿å­˜è·¯å¾„
        epochs: è®­ç»ƒè½®æ•°
        sequence_length: åºåˆ—é•¿åº¦
    """
    from skills.models.attention_lstm import AttentionLSTMModel
    from skills.indicators.technical import IndicatorFeatures
    from skills.strategy.ml_strategy import FeatureEngineer

    print("=" * 60)
    print("è®­ç»ƒ Attention-LSTM æ¨¡å‹")
    print("=" * 60)

    # åŠ è½½æ•°æ®
    print(f"\nğŸ“Š åŠ è½½æ•°æ®: {data_path}")
    bars = load_real_data(data_path)
    print(f"   âœ“ åŠ è½½ {len(bars)} æ¡ K çº¿")

    # æå–ç‰¹å¾ï¼ˆä½¿ç”¨æ–°çš„æŠ€æœ¯æŒ‡æ ‡åº“ï¼‰
    print("\nğŸ”§ æå–æŠ€æœ¯æŒ‡æ ‡ç‰¹å¾...")
    indicator_features = IndicatorFeatures.extract_all(bars)

    # è½¬æ¢ä¸ºç‰¹å¾çŸ©é˜µ
    feature_list = []
    for i in range(len(bars)):
        row = []
        for name, values in indicator_features.items():
            if i < len(values):
                val = values[i]
                if float('nan') == val:
                    # ä½¿ç”¨å‰ä¸€ä¸ªæœ‰æ•ˆå€¼
                    for j in range(i-1, -1, -1):
                        if j < len(values) and not float('nan') == values[j]:
                            val = values[j]
                            break
                    else:
                        val = 0.0
                else:
                    val = float(val)
                row.append(val)
            else:
                row.append(0.0)
        feature_list.append(row)

    features = np.array(feature_list)
    print(f"   âœ“ ç‰¹å¾å½¢çŠ¶: {features.shape}")

    # ç”Ÿæˆæ ‡ç­¾
    print("\nğŸ·ï¸  ç”Ÿæˆæ ‡ç­¾...")
    labels, horizon = generate_labels(bars, sequence_length, prediction_horizon=5)
    print(f"   âœ“ æ ‡ç­¾æ•°é‡: {len(labels)}")
    print(f"   âœ“ ç±»åˆ«åˆ†å¸ƒ: è·Œ={sum(labels==0)}, å¹³={sum(labels==1)}, æ¶¨={sum(labels==2)}")

    # å¯¹é½æ•°æ®
    min_len = min(len(features), len(labels))
    features = features[:min_len]
    labels = labels[:min_len]

    # é‡å¡‘ä¸º LSTM æ ¼å¼
    print(f"\nğŸ“ é‡å¡‘æ•°æ®ä¸º LSTM æ ¼å¼...")
    features_per_step = features.shape[1] // sequence_length
    X = features[:, :sequence_length * features_per_step].reshape(
        -1, sequence_length, features_per_step
    )
    y = labels[:len(X)]

    print(f"   âœ“ X.shape: {X.shape}")
    print(f"   âœ“ y.shape: {y.shape}")

    # åˆ›å»ºæ¨¡å‹
    print(f"\nğŸ¤– åˆ›å»º Attention-LSTM æ¨¡å‹...")
    model = AttentionLSTMModel(
        sequence_length=sequence_length,
        lstm_units=128,
        attention_units=64,
        dropout_rate=0.3,
        bidirectional=True
    )

    # è®­ç»ƒ
    print(f"\nğŸš€ å¼€å§‹è®­ç»ƒ ({epochs} epochs)...\n")
    history = model.train(X, y, epochs=epochs, batch_size=32)

    # ä¿å­˜æ¨¡å‹
    print(f"\nğŸ’¾ ä¿å­˜æ¨¡å‹: {model_path}")
    model.save(model_path)

    # æ‰“å°æœ€ç»ˆæŒ‡æ ‡
    final_loss = history.history['loss'][-1]
    final_acc = history.history['accuracy'][-1]
    final_val_loss = history.history['val_loss'][-1]
    final_val_acc = history.history['val_accuracy'][-1]

    print(f"\nğŸ“Š æœ€ç»ˆæŒ‡æ ‡:")
    print(f"   è®­ç»ƒæŸå¤±: {final_loss:.4f}")
    print(f"   è®­ç»ƒå‡†ç¡®ç‡: {final_acc:.4f}")
    print(f"   éªŒè¯æŸå¤±: {final_val_loss:.4f}")
    print(f"   éªŒè¯å‡†ç¡®ç‡: {final_val_acc:.4f}")

    print("\nâœ… è®­ç»ƒå®Œæˆ!")
    return model


def load_real_data(path):
    """åŠ è½½çœŸå®æ•°æ®"""
    import csv
    from decimal import Decimal
    from core.models import Bar

    bars = []
    with open(path, 'r') as f:
        reader = csv.DictReader(f)
        for row in reader:
            bar = Bar(
                symbol=row['symbol'],
                timestamp=datetime.fromisoformat(row['timestamp']),
                open=Decimal(row['open']),
                high=Decimal(row['high']),
                low=Decimal(row['low']),
                close=Decimal(row['close']),
                volume=int(row['volume']),
                interval="1d"
            )
            bars.append(bar)

    bars.sort(key=lambda x: x.timestamp)
    return bars


def generate_labels(bars, min_bars, prediction_horizon=5):
    """ç”Ÿæˆè®­ç»ƒæ ‡ç­¾"""
    labels = []

    for i in range(min_bars, len(bars) - prediction_horizon):
        current_price = float(bars[i].close)
        future_price = float(bars[i + prediction_horizon].close)
        change = (future_price - current_price) / current_price

        if change > 0.01:  # æ¶¨è¶…è¿‡1%
            labels.append(2)
        elif change < -0.01:  # è·Œè¶…è¿‡1%
            labels.append(0)
        else:
            labels.append(1)

    return np.array(labels), prediction_horizon


def main():
    import argparse

    parser = argparse.ArgumentParser(description='è®­ç»ƒ Attention-LSTM æ¨¡å‹')
    parser.add_argument('--data-path', default='data/real_market_data.csv')
    parser.add_argument('--model-path', default='models/attention_lstm.h5')
    parser.add_argument('--epochs', type=int, default=100)
    parser.add_argument('--sequence-length', type=int, default=30)

    args = parser.parse_args()

    train_attention_lstm(
        data_path=args.data_path,
        model_path=args.model_path,
        epochs=args.epochs,
        sequence_length=args.sequence_length
    )


if __name__ == "__main__":
    main()
